import os
import json
from redis import Redis
import numpy as np
from dotenv import load_dotenv
from redis.commands.search.query import Query
from redis.commands.search.field import TagField, VectorField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
from redis.commands.search.query import Query
from src.utils.logger import logging as logger
from src.llm.embeddings.text_embeddings import TextEmbeddings

load_dotenv()

REDIS_URL = str(os.getenv("REDIS_URL"))
INDEX_NAME = "prompt_completion"  # Vector Index Name
DOC_PREFIX = "doc:"  # RediSearch Key Prefix for the Index
D_MODEL = int(os.getenv("D_MODEL"))


class RedisDB:

    def __init__(self) -> None:
        """the redis sdk creates a standard way to cache prompts and completions"""
        try:
            logger.info(f"inititalizing the redis connection at { REDIS_URL }")
            self.client = Redis.from_url(REDIS_URL)
            self.prompt_completion_index = self.create_index(D_MODEL)
            logger.info("inititating the redis pipeline")
        except RuntimeError as e:
            logger.error("can not cache the completion from the llm")
            logger.error(e)

        self.pipe = self.client.pipeline()
        self.text_embedding_model = TextEmbeddings().embed_model

    def put(self, prompt, completion, score):
        """
        adds the embedding of the prompt sinto the the db with the prompt as the key and the value having the following
          1. prompt - the prompt input to the llm
          2. completion - the completion from the llm
          3. score - the score generated by the llm
          4. prompt_embedding - the embedding for the last token of the prompt
        """
        try:
            objects = {"prompt": prompt, "completion": completion, "score": score}
            logger.info(f"adding object {json.dumps(objects)} to redis db")
            key = f"doc:{objects['prompt']}"
            objects["prompt_embedding"] = (
                np.array(self.text_embedding_model.get_text_embedding(prompt))
                .astype(dtype=np.float32)
                .tobytes()
            )
            self.pipe.hset(key, mapping=objects)
            res = self.pipe.execute()
            return res
        except RuntimeError as e:
            logger.error("can not put the completion from the llm ")
            logger.error(e)

    def get_top_k(self, prompt, top_k):
        """
        get the prompt and the query for the top K similar items based on
        the embedding of the last token then sort the result by cosine distance
        to get the top k similar prompt embeddings
        """
        try:
            query = (
                Query(
                    f"*=>[KNN {top_k} @prompt_embedding $vec]=>{{$yield_distance_as: dist}}"
                )
                .sort_by(f"dist")
                .return_fields("prompt", "completion", "score", "dist")
                .dialect(2)
            )
            query_params = {
                "vec": np.array(self.text_embedding_model.get_text_embedding(prompt))
                .astype(dtype=np.float32)
                .tobytes()
            }
            result = self.client.ft(INDEX_NAME).search(query, query_params)
            return result.docs
        except RuntimeError as e:
            logger.error("can not get k nearest from cache getting from llm")
            logger.error(e)

    def create_index(self, vector_dimensions: int):
        logger.info(f"creating index {INDEX_NAME} in redis")
        try:
            self.client.ft(INDEX_NAME).info()
            logger.error("Index already exists!")
        except:
            schema = (
                TagField("prompt"),
                TagField("completion"),
                TagField("score"),  # Tag Field Name
                VectorField(
                    "prompt_embedding",  # Vector Field Name
                    "FLAT",
                    {  # Vector Index Type: FLAT or HNSW
                        "TYPE": "FLOAT32",  # FLOAT32 or FLOAT64
                        "DIM": vector_dimensions,  # Number of Vector Dimensions
                        "DISTANCE_METRIC": "COSINE",  # Vector Search Distance Metric
                    },
                ),
            )

            definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)
            index = self.client.ft(INDEX_NAME).create_index(
                fields=schema, definition=definition
            )
            logger.info(f"created index {INDEX_NAME} in redis")
            return index
