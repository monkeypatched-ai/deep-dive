""" wrapper for redis"""
# pylint: disable=line-too-long,import-error,no-self-use,inconsistent-return-statements,no-name-in-module,import-self

import os
import json
from redis import Redis
from redis.commands.search.query import Query
from redis.commands.search.field import TagField, VectorField
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
import numpy as np
from dotenv import load_dotenv
from src.utils.logger import logging as logger
from src.llm.embeddings.text_embeddings import TextEmbeddings

load_dotenv()

REDIS_URL = str(os.getenv("REDIS_URL"))
INDEX_NAME = "prompt_completion"  # Vector Index Name
DOC_PREFIX = "doc:"  # RediSearch Key Prefix for the Index
D_MODEL = int(os.getenv("D_MODEL"))


class RedisDB:
    """ wrapper for redis vector db"""

    def __init__(self) -> None:
        """the redis sdk creates a standard way to cache prompts and completions"""
        try:
            logger.info(f"inititalizing the redis connection at { REDIS_URL }")
            self.client = Redis.from_url(REDIS_URL)
            self.prompt_completion_index = self.create_index(D_MODEL)
            logger.info("inititating the redis pipeline")
        except RuntimeError as error:
            logger.error("can not cache the completion from the llm")
            logger.error(error)

        self.pipe = self.client.pipeline()
        self.text_embedding_model = TextEmbeddings().embed_model

    def put(self, prompt, completion, score):
        """
        adds the embedding of the prompt sinto the the db with the prompt as the key and the value having the following
          1. prompt - the prompt input to the llm
          2. completion - the completion from the llm
          3. score - the score generated by the llm
          4. prompt_embedding - the embedding for the last token of the prompt
        """
        try:
            objects = {"prompt": prompt, "completion": completion, "score": score}
            logger.info(f"adding object {json.dumps(objects)} to redis db")
            key = f"doc:{objects['prompt']}"
            objects["prompt_embedding"] = (
                np.array(self.text_embedding_model.get_text_embedding(prompt))
                .astype(dtype=np.float32)
                .tobytes()
            )
            self.pipe.hset(key, mapping=objects)
            res = self.pipe.execute()
            return res
        except RuntimeError as error:
            logger.error("can not put the completion from the llm ")
            logger.error(error)

    def get_top_k(self, prompt, top_k):
        """
        get the prompt and the query for the top K similar items based on
        the embedding of the last token then sort the result by cosine distance
        to get the top k similar prompt embeddings
        """
        try:
            query = (
                Query(
                    f"*=>[KNN {top_k} @prompt_embedding $vec]=>{{$yield_distance_as: dist}}"
                )
                .sort_by("dist")
                .return_fields("prompt", "completion", "score", "dist")
                .dialect(2)
            )
            query_params = {
                "vec": np.array(self.text_embedding_model.get_text_embedding(prompt))
                .astype(dtype=np.float32)
                .tobytes()
            }
            result = self.client.ft(INDEX_NAME).search(query, query_params)
            return result.docs
        except RuntimeError as error:
            logger.error("can not get k nearest from cache getting from llm")
            logger.error(error)

    def create_index(self, vector_dimensions: int):
        """ this function is used to create the index on redis dbs """
        logger.info(f"creating index {INDEX_NAME} in redis")
        try:
            self.client.ft(INDEX_NAME).info()
            logger.error("Index already exists!")
        except RuntimeError as error:
            schema = (
                TagField("prompt"),
                TagField("completion"),
                TagField("score"),  # Tag Field Name
                VectorField(
                    "prompt_embedding",  # Vector Field Name
                    "FLAT",
                    {  # Vector Index Type: FLAT or HNSW
                        "TYPE": "FLOAT32",  # FLOAT32 or FLOAT64
                        "DIM": vector_dimensions,  # Number of Vector Dimensions
                        "DISTANCE_METRIC": "COSINE",  # Vector Search Distance Metric
                    },
                ),
            )

            definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)
            index = self.client.ft(INDEX_NAME).create_index(
                fields=schema, definition=definition
            )
            logger.info(f"created index {INDEX_NAME} in redis")
            logger.error(error)
            return index
